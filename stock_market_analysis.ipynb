{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Share Market & Amazon Product Rating Analysis\n",
"This notebook performs:\n",
"1. Amazon product rating prediction using a neural network.\n",
"2. Stock market dataset loading, preprocessing, and basic analysis."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": ["## 1️⃣ Amazon Product Rating Prediction"]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"import re\n",
"import os\n",
"import math\n",
"import numpy as np\n",
"import pandas as pd\n",
"import matplotlib.pyplot as plt\n",
"from sklearn.model_selection import train_test_split\n",
"from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
"from sklearn.impute import SimpleImputer\n",
"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
"from sklearn.pipeline import Pipeline\n",
"from sklearn.compose import ColumnTransformer\n",
"\n",
"# File and model params\n",
"FILE_NAME = '/content/sample_data/amazon.csv'\n",
"TARGET = 'rating'\n",
"EPOCHS = 80\n",
"BATCH_SIZE = 32\n",
"SAVE_MODEL = True\n",
"RANDOM_STATE = 42\n",
"\n",
"def to_float_price(x):\n",
"    if pd.isna(x): return np.nan\n",
"    if isinstance(x, (int, float)): return float(x)\n",
"    s = str(x).strip()\n",
"    s = re.sub(r'[^\d\.\-]', '', s)\n",
"    return float(s) if s else np.nan\n",
"\n",
"def to_float_percent(x):\n",
"    if pd.isna(x): return np.nan\n",
"    if isinstance(x, (int, float)): return float(x)\n",
"    s = str(x).replace('%','').strip()\n",
"    s = re.sub(r'[^\d\.\-]', '', s)\n",
"    return float(s) if s else np.nan\n",
"\n",
"def to_float_generic(x):\n",
"    if pd.isna(x): return np.nan\n",
"    if isinstance(x, (int, float)): return float(x)\n",
"    s = re.sub(r'[^\d\.\-]', '', str(x))\n",
"    return float(s) if s else np.nan\n",
"\n",
"# Load dataset\n",
"if not os.path.exists(FILE_NAME):\n",
"    raise FileNotFoundError(f'File not found: {FILE_NAME}')\n",
"\n",
"df = pd.read_csv(FILE_NAME)\n",
"print('Dataset Loaded Successfully')\n",
"print('Columns:', df.columns.tolist())\n",
"\n",
"if TARGET not in df.columns:\n",
"    raise ValueError(f'Target column {TARGET} not found')\n",
"\n",
"data = df.copy()\n",
"\n",
"# Convert numeric columns\n",
"for col in ['discounted_price','actual_price']:\n",
"    if col in data.columns: data[col] = data[col].apply(to_float_price)\n",
"if 'discount_percentage' in data.columns: data['discount_percentage'] = data['discount_percentage'].apply(to_float_percent)\n",
"if 'rating_count' in data.columns: data['rating_count'] = data['rating_count'].apply(to_float_generic)\n",
"\n",
"data[TARGET] = data[TARGET].apply(to_float_generic)\n",
"initial_len = len(data)\n",
"data = data.dropna(subset=[TARGET])\n",
"print(f'Dropped {initial_len - len(data)} rows with missing target')\n",
"\n",
"numeric_candidates = ['discounted_price','actual_price','discount_percentage','rating_count']\n",
"numeric_features = [c for c in numeric_candidates if c in data.columns]\n",
"cat_features = ['category'] if 'category' in data.columns else []\n",
"\n",
"# Top-K category mapping\n",
"MAX_CATEGORIES=40\n",
"if 'category' in cat_features:\n",
"    top_cats = data['category'].value_counts().nlargest(MAX_CATEGORIES).index\n",
"    data['category'] = data['category'].where(data['category'].isin(top_cats),'OTHER')\n",
"    print('Category unique after top-K mapping:', data['category'].nunique())\n",
"\n",
"features = numeric_features + cat_features\n",
"if not features: raise ValueError('No usable features found.')\n",
"\n",
"X = data[features].copy()\n",
"y = data[TARGET].astype(float).copy()\n",
"\n",
"numeric_transformer = Pipeline([('imputer',SimpleImputer(strategy='median')),('scaler',MinMaxScaler())])\n",
"cat_transformer = Pipeline([('imputer',SimpleImputer(strategy='constant',fill_value='UNKNOWN')),('onehot',OneHotEncoder(handle_unknown='ignore',sparse_output=False))]) if cat_features else None\n",
"transformers = [('num',numeric_transformer,numeric_features)] + ([('cat',cat_transformer,cat_features)] if cat_features else [])\n",
"preprocessor = ColumnTransformer(transformers=transformers,remainder='drop')\n",
"X_pre = preprocessor.fit_transform(X)\n",
"print('Preprocessed feature shape:', X_pre.shape)\n",
"\n",
"X_train,X_test,y_train,y_test = train_test_split(X_pre,y,test_size=0.2,random_state=RANDOM_STATE)\n",
"print('Train/Test sizes:', X_train.shape[0], X_test.shape[0])\n",
"\n",
"# TensorFlow Model\n",
"USE_TF=True\n",
"try:\n",
"    import tensorflow as tf\n",
"    from tensorflow.keras.models import Sequential\n",
"    from tensorflow.keras.layers import Dense,Dropout\n",
"except Exception:\n",
"    USE_TF=False\n",
"    print('TF not available. Using sklearn MLP.')\n",
"\n",
"if USE_TF:\n",
"    model = Sequential([Dense(256,activation='relu',input_shape=(X_train.shape[1],)),Dropout(0.3),Dense(128,activation='relu'),Dropout(0.2),Dense(64,activation='relu'),Dense(1)])\n",
"    model.compile(optimizer='adam',loss='mse',metrics=['mae'])\n",
"    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=8,restore_best_weights=True)]\n",
"    history = model.fit(X_train,y_train,validation_split=0.1,epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=callbacks,verbose=2)\n",
"    y_pred = model.predict(X_test).flatten()\n",
"    if SAVE_MODEL: model.save('amazon_rating_model_tf.h5')\n",
"else:\n",
"    from sklearn.neural_network import MLPRegressor\n",
"    mlp = MLPRegressor(hidden_layer_sizes=(200,100),activation='relu',solver='adam',max_iter=EPOCHS,random_state=RANDOM_STATE,verbose=True,early_stopping=True,validation_fraction=0.1)\n",
"    mlp.fit(X_train,y_train)\n",
"    y_pred = mlp.predict(X_test)\n",
"    if SAVE_MODEL: import joblib; joblib.dump(mlp,'amazon_rating_model_sklearn.joblib')\n",
"\n",
"mae = mean_absolute_error(y_test,y_pred)\n",
"mse = mean_squared_error(y_test,y_pred)\n",
"rmse = math.sqrt(mse)\n",
"r2 = r2_score(y_test,y_pred)\n",
"print(f'MAE={mae}, MSE={mse}, RMSE={rmse}, R2={r2}')\n",
"\n",
"plt.figure(figsize=(10,4))\n",
"N=min(300,len(y_test))\n",
"plt.plot(range(N),y_test.values[:N],label='actual',marker='.',linewidth=1)\n",
"plt.plot(range(N),y_pred[:N],label='predicted',marker='.',linewidth=1)\n",
"plt.legend()\n",
"plt.title('Actual vs Predicted Ratings')\n",
"plt.show()"
},
{
"cell_type": "markdown",
"metadata": {},
"source": ["## 2️⃣ Stock Market Analysis"]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"STOCK_FILE = '/content/sample_data/stock_market.csv'\n",
"if not os.path.exists(STOCK_FILE): raise FileNotFoundError(f'Stock market dataset not found: {STOCK_FILE}')\n",
"stock_df = pd.read_csv(STOCK_FILE)\n",
"print(stock_df.head())\n",
"if 'Date' in stock_df.columns: stock_df['Date'] = pd.to_datetime(stock_df['Date']); stock_df.sort_values('Date',inplace=True)\n",
"if 'Close' in stock_df.columns:\n",
"    plt.figure(figsize=(12,5))\n",
"    plt.plot(stock_df['Date'],stock_df['Close'],label='Close Price')\n",
"    plt.title('Stock Close Price Over Time')\n",
"    plt.legend()\n",
"    plt.show()\n",
"    stock_df['Returns'] = stock_df['Close'].pct_change()\n",
"    plt.figure(figsize=(12,4))\n",
"    plt.plot(stock_df['Date'],stock_df['Returns'],label='Daily Returns')\n",
"    plt.title('Daily Returns')\n",
"    plt.legend()\n",
"    plt.show()\n",
"import seaborn as sns\n",
"plt.figure(figsize=(8,6))\n",
"sns.heatmap(stock_df.corr(),annot=True,cmap='coolwarm')\n",
"plt.title('Stock Feature Correlations')\n",
"plt.show()"
]
}
],
"metadata": {"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.12"}},
"nbformat": 4,
"nbformat_minor": 5
}


